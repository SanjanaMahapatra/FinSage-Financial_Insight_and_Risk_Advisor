{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a8482e58",
      "metadata": {
        "id": "a8482e58"
      },
      "source": [
        "## Performing Fraud Detection analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "RgGQ0cv8dCLK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgGQ0cv8dCLK",
        "outputId": "3a83cca2-fc5b-46af-be34-467d9a85b2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files available in this notebook ===> \n",
            "dirname ==> \n",
            " ../data/raw/transaction_kaggle_dataset/\n",
            "filename ==> \n",
            " cards_data.csv\n",
            "../data/raw/transaction_kaggle_dataset/cards_data.csv\n",
            "dirname ==> \n",
            " ../data/raw/transaction_kaggle_dataset/\n",
            "filename ==> \n",
            " mcc_codes.json\n",
            "../data/raw/transaction_kaggle_dataset/mcc_codes.json\n",
            "dirname ==> \n",
            " ../data/raw/transaction_kaggle_dataset/\n",
            "filename ==> \n",
            " train_fraud_labels.json\n",
            "../data/raw/transaction_kaggle_dataset/train_fraud_labels.json\n",
            "dirname ==> \n",
            " ../data/raw/transaction_kaggle_dataset/\n",
            "filename ==> \n",
            " transactions_data.csv\n",
            "../data/raw/transaction_kaggle_dataset/transactions_data.csv\n",
            "dirname ==> \n",
            " ../data/raw/transaction_kaggle_dataset/\n",
            "filename ==> \n",
            " users_data.csv\n",
            "../data/raw/transaction_kaggle_dataset/users_data.csv\n",
            "\n",
            "Loading data files...\n",
            "\n",
            "All data files loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Standard imports for data manipulation\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import gc\n",
        "import re\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "print(\"Files available in this notebook ===> \")\n",
        "\n",
        "DATASET_PATH = '../data/raw/transaction_kaggle_dataset/'\n",
        "\n",
        "for dirname, _, filenames in os.walk(DATASET_PATH):\n",
        "  for filename in filenames:\n",
        "    print(\"dirname ==> \\n\", dirname)\n",
        "    print(\"filename ==> \\n\", filename)\n",
        "    print(os.path.join(dirname, filename))\n",
        "\n",
        "\n",
        "print(\"\\nLoading data files...\")\n",
        "try:\n",
        "  # Load fraud labels from JSON\n",
        "  file_path_fraud_labels = DATASET_PATH + 'train_fraud_labels.json'\n",
        "\n",
        "  with open(file_path_fraud_labels, 'r') as file:\n",
        "\n",
        "    if os.path.getsize(file_path_fraud_labels) > 0:\n",
        "      raw_json_data = json.load(file)\n",
        "    else:\n",
        "      # Handle the empty file case, e.g., by providing a default value\n",
        "      raw_json_data = {'target': {}}\n",
        "      print(f\"Warning: The file '{file_path_fraud_labels}' is empty. Using default data.\")\n",
        "\n",
        "  transaction_labels_dict = raw_json_data['target']\n",
        "  train_fraud_labels = pd.Series(transaction_labels_dict).reset_index()\n",
        "  train_fraud_labels.columns = ['transaction_id', 'is_fraud']\n",
        "  train_fraud_labels['transaction_id'] = pd.to_numeric(train_fraud_labels['transaction_id'])\n",
        "\n",
        "  # Load other data files\n",
        "  transaction_df = pd.read_csv(DATASET_PATH + 'transactions_data.csv')\n",
        "  card_df = pd.read_csv(DATASET_PATH + 'cards_data.csv')\n",
        "  users_df = pd.read_csv(DATASET_PATH + \"users_data.csv\")\n",
        "  mcc_series = pd.read_json(DATASET_PATH + 'mcc_codes.json', typ='series')\n",
        "  mcc_df = mcc_series.reset_index()\n",
        "  mcc_df.columns = ['mcc_code', 'description']\n",
        "\n",
        "  print(\"\\nAll data files loaded successfully.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "  print(\"\\nERROR: FileNotFoundError. Please ensure the dataset is attached to the notebook (using the '+ Add data' button on the right) and the DATASET_PATH is correct.\")\n",
        "\n",
        "except json.JSONDecodeError as e:\n",
        "  print(f\"JSON decoding failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0bwT35HgdCLL",
      "metadata": {
        "id": "0bwT35HgdCLL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All data merged into a single DataFrame and original tables deleted.\n",
            "Shape of the final merged DataFrame: (13305915, 39)\n"
          ]
        }
      ],
      "source": [
        "# --- Merge all DataFrames into one comprehensive DataFrame ---\n",
        "\n",
        "# Start with the main transactions data\n",
        "df = pd.merge(transaction_df, train_fraud_labels, left_on='id', right_on='transaction_id', how='left')\n",
        "\n",
        "# Merge with card data (card_df.id is the card_id)\n",
        "# Using suffixes to handle potential duplicate column names (e.g., 'id' in both)\n",
        "df = pd.merge(df, card_df, left_on='card_id', right_on='id', how='left', suffixes=('', '_card'))\n",
        "\n",
        "# Merge with user data (users_df.id is the client_id)\n",
        "# Using suffixes again for robustness\n",
        "df = pd.merge(df, users_df, left_on='client_id', right_on='id', how='left', suffixes=('', '_user'))\n",
        "\n",
        "# Merge with MCC descriptions (mcc_df.mcc_code is the mcc)\n",
        "# Note: 'mcc' is the code in transaction_df, 'mcc_code' is the code in mcc_df\n",
        "df = pd.merge(df, mcc_df, left_on='mcc', right_on='mcc_code', how='left')\n",
        "\n",
        "# --- Clean up merged columns ---\n",
        "# Drop redundant ID columns from the merges\n",
        "# 'transaction_id' is redundant with 'id' (from transaction_df)\n",
        "# 'id_card' is redundant with 'card_id'\n",
        "# 'id_user' is redundant with 'client_id'\n",
        "# 'mcc_code' is redundant with 'mcc'\n",
        "df = df.drop(columns=['transaction_id', 'id_card', 'id_user', 'mcc_code'])\n",
        "\n",
        "# Delete original dataframes to free up memory\n",
        "del transaction_df, train_fraud_labels, card_df, users_df, mcc_df\n",
        "gc.collect() # Manually trigger garbage collection\n",
        "\n",
        "print(\"All data merged into a single DataFrame and original tables deleted.\")\n",
        "print(f\"Shape of the final merged DataFrame: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "23d97b53",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>client_id</th>\n",
              "      <th>card_id</th>\n",
              "      <th>amount</th>\n",
              "      <th>use_chip</th>\n",
              "      <th>merchant_id</th>\n",
              "      <th>merchant_city</th>\n",
              "      <th>merchant_state</th>\n",
              "      <th>zip</th>\n",
              "      <th>...</th>\n",
              "      <th>gender</th>\n",
              "      <th>address</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>per_capita_income</th>\n",
              "      <th>yearly_income</th>\n",
              "      <th>total_debt</th>\n",
              "      <th>credit_score</th>\n",
              "      <th>num_credit_cards</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7475327</td>\n",
              "      <td>2010-01-01 00:01:00</td>\n",
              "      <td>1556</td>\n",
              "      <td>2972</td>\n",
              "      <td>$-77.00</td>\n",
              "      <td>Swipe Transaction</td>\n",
              "      <td>59935</td>\n",
              "      <td>Beulah</td>\n",
              "      <td>ND</td>\n",
              "      <td>58523.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Female</td>\n",
              "      <td>594 Mountain View Street</td>\n",
              "      <td>46.8</td>\n",
              "      <td>-100.76</td>\n",
              "      <td>$23679</td>\n",
              "      <td>$48277</td>\n",
              "      <td>$110153</td>\n",
              "      <td>740</td>\n",
              "      <td>4</td>\n",
              "      <td>Miscellaneous Food Stores</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7475328</td>\n",
              "      <td>2010-01-01 00:02:00</td>\n",
              "      <td>561</td>\n",
              "      <td>4575</td>\n",
              "      <td>$14.57</td>\n",
              "      <td>Swipe Transaction</td>\n",
              "      <td>67570</td>\n",
              "      <td>Bettendorf</td>\n",
              "      <td>IA</td>\n",
              "      <td>52722.0</td>\n",
              "      <td>...</td>\n",
              "      <td>Male</td>\n",
              "      <td>604 Pine Street</td>\n",
              "      <td>40.8</td>\n",
              "      <td>-91.12</td>\n",
              "      <td>$18076</td>\n",
              "      <td>$36853</td>\n",
              "      <td>$112139</td>\n",
              "      <td>834</td>\n",
              "      <td>5</td>\n",
              "      <td>Department Stores</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows Ã— 39 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                 date  client_id  card_id   amount  \\\n",
              "0  7475327  2010-01-01 00:01:00       1556     2972  $-77.00   \n",
              "1  7475328  2010-01-01 00:02:00        561     4575   $14.57   \n",
              "\n",
              "            use_chip  merchant_id merchant_city merchant_state      zip  ...  \\\n",
              "0  Swipe Transaction        59935        Beulah             ND  58523.0  ...   \n",
              "1  Swipe Transaction        67570    Bettendorf             IA  52722.0  ...   \n",
              "\n",
              "   gender                   address latitude  longitude per_capita_income  \\\n",
              "0  Female  594 Mountain View Street     46.8    -100.76            $23679   \n",
              "1    Male           604 Pine Street     40.8     -91.12            $18076   \n",
              "\n",
              "  yearly_income  total_debt credit_score  num_credit_cards  \\\n",
              "0        $48277     $110153          740                 4   \n",
              "1        $36853     $112139          834                 5   \n",
              "\n",
              "                 description  \n",
              "0  Miscellaneous Food Stores  \n",
              "1          Department Stores  \n",
              "\n",
              "[2 rows x 39 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6aB_ZC07dCLL",
      "metadata": {
        "id": "6aB_ZC07dCLL"
      },
      "outputs": [],
      "source": [
        "# Drop rows where the fraud label is missing (these are unlabeled transactions)\n",
        "# This is a critical step as supervised learning requires labeled data.\n",
        "df.dropna(subset=['is_fraud'], inplace=True)\n",
        "\n",
        "# Convert the target variable 'is_fraud' to a numerical format (0 for 'No', 1 for 'Yes')\n",
        "# This is required by most machine learning algorithms.\n",
        "df['is_fraud'] = df['is_fraud'].map({'No': 0, 'Yes': 1})\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "features = [col for col in df.columns if col != 'is_fraud']\n",
        "X = df[features]\n",
        "y = df['is_fraud']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "QAoxrl-xdCLM",
      "metadata": {
        "id": "QAoxrl-xdCLM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full dataset split into training, validation, and test sets.\n",
            "X_train shape: (5348977, 38)\n",
            "X_cv shape: (1782993, 38)\n",
            "X_test shape: (1782993, 38)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training (60%), and a temporary set (40%)\n",
        "# The first split takes 40% for temp, leaving 60% for X_train.\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.40, random_state=42, stratify=y)\n",
        "\n",
        "# Split the temporary set (which is 40% of original) into validation (20% of original) and test (20% of original)\n",
        "# 0.50 of X_temp (40%) = 20% of original.\n",
        "X_cv, X_test, y_cv, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Delete the large intermediate dataframes to save memory (important for Kaggle kernels)\n",
        "del df, X, y, X_temp, y_temp\n",
        "gc.collect() # Manually trigger garbage collection\n",
        "\n",
        "print(\"Full dataset split into training, validation, and test sets.\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_cv shape: {X_cv.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9kk4RlXbdJiE",
      "metadata": {
        "id": "9kk4RlXbdJiE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature engineering function 'create_all_features' defined.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def apply_preprocessing(df, is_training_set=False, median_imputations=None):\n",
        "    \"\"\"\n",
        "    Takes a raw data split and applies all feature engineering steps.\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # --- Step 1: Clean Numerical Columns ---\n",
        "    amount_cols = ['amount', 'per_capita_income', 'yearly_income', 'credit_limit', 'total_debt']\n",
        "    for col in amount_cols:\n",
        "        if col in df_processed.columns:\n",
        "            df_processed[col] = pd.to_numeric(df_processed[col].astype(str).str.replace(r'[$,]', '', regex=True), errors='coerce')\n",
        "\n",
        "    # --- Step 2: Date Engineering ---\n",
        "    date_cols = ['date', 'expires', 'acct_open_date']\n",
        "    for col in date_cols:\n",
        "        if col in df_processed.columns:\n",
        "            df_processed[col] = pd.to_datetime(df_processed[col], errors='coerce', format='mixed')\n",
        "\n",
        "    if 'date' in df_processed.columns:\n",
        "        df_processed['hour_of_day'] = df_processed['date'].dt.hour\n",
        "        df_processed['day_of_week'] = df_processed['date'].dt.dayofweek\n",
        "        df_processed['month'] = df_processed['date'].dt.month\n",
        "    if 'expires' in df_processed.columns and 'date' in df_processed.columns:\n",
        "        df_processed['days_to_expiry'] = (df_processed['expires'] - df_processed['date']).dt.days\n",
        "\n",
        "    # Drop original date columns right after use, as you suggested\n",
        "    df_processed.drop(columns=date_cols, inplace=True)\n",
        "\n",
        "    # --- Step 3: Cyclical Feature Creation ---\n",
        "    cyclical_cols_original = ['hour_of_day', 'day_of_week', 'month']\n",
        "    if all(col in df_processed.columns for col in cyclical_cols_original):\n",
        "        df_processed['hour_sin'] = np.sin(2 * np.pi * df_processed['hour_of_day'] / 24.0)\n",
        "        df_processed['hour_cos'] = np.cos(2 * np.pi * df_processed['hour_of_day'] / 24.0)\n",
        "        df_processed['day_of_week_sin'] = np.sin(2 * np.pi * df_processed['day_of_week'] / 7.0)\n",
        "        df_processed['day_of_week_cos'] = np.cos(2 * np.pi * df_processed['day_of_week'] / 7.0)\n",
        "        df_processed['month_sin'] = np.sin(2 * np.pi * df_processed['month'] / 12.0)\n",
        "        df_processed['month_cos'] = np.cos(2 * np.pi * df_processed['month'] / 12.0)\n",
        "\n",
        "        # Drop original cyclical columns right after use, as you suggested\n",
        "        df_processed.drop(columns=cyclical_cols_original, inplace=True)\n",
        "\n",
        "    # --- Step 4: Process Binary and Other Features ---\n",
        "    if 'errors' in df_processed.columns:\n",
        "        df_processed['has_error'] = df_processed['errors'].notna().astype(int)\n",
        "    if 'gender' in df_processed.columns:\n",
        "        df_processed['gender'] = df_processed['gender'].map({'Female': 0, 'Male': 1})\n",
        "    if 'has_chip' in df_processed.columns:\n",
        "        df_processed['has_chip'] = df_processed['has_chip'].map({'NO': 0, 'YES': 1})\n",
        "\n",
        "    # --- Step 5: Final NaN Imputation for numerical features (within this function) ---\n",
        "    # This is done after all numerical features are created.\n",
        "    # Medians will be calculated from X_train when this function is called on X_train.\n",
        "    # Then passed to X_cv and X_test calls.\n",
        "    numerical_cols_for_imputation = df_processed.select_dtypes(include=np.number).columns.tolist()\n",
        "    if is_training_set:\n",
        "        median_imputations = df_processed[numerical_cols_for_imputation].median()\n",
        "\n",
        "    if median_imputations is not None:\n",
        "        df_processed.fillna(median_imputations, inplace=True)\n",
        "\n",
        "    return df_processed, median_imputations # Return processed DF and medians\n",
        "\n",
        "print(\"Feature engineering function 'create_all_features' defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "w-YwWVjGdJfP",
      "metadata": {
        "id": "w-YwWVjGdJfP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Applying feature engineering to all data splits...\n",
            "Stage 1: Feature Engineering applied to all splits.\n",
            "X_train shape after initial FE: (5348977, 43)\n",
            "X_train columns after initial FE (first 10): ['id', 'client_id', 'card_id', 'amount', 'use_chip', 'merchant_id', 'merchant_city', 'merchant_state', 'zip', 'mcc']...\n"
          ]
        }
      ],
      "source": [
        "# --- Main preprocessing execution block ---\n",
        "print(\"Step 1: Applying feature engineering to all data splits...\")\n",
        "\n",
        "# Initialize median_imputations_dict outside the loop; it will be populated by X_train's processing\n",
        "median_imputations_dict = None\n",
        "\n",
        "# Apply preprocessing to X_train (where medians for imputation are learned)\n",
        "X_train, median_imputations_dict = apply_preprocessing(X_train, is_training_set=True)\n",
        "# Apply preprocessing to X_cv and X_test (using medians learned from X_train)\n",
        "X_cv, _ = apply_preprocessing(X_cv, median_imputations=median_imputations_dict)\n",
        "X_test, _ = apply_preprocessing(X_test, median_imputations=median_imputations_dict)\n",
        "gc.collect() # Clean up memory\n",
        "\n",
        "print('Stage 1: Feature Engineering applied to all splits.')\n",
        "print(f\"X_train shape after initial FE: {X_train.shape}\")\n",
        "print(f\"X_train columns after initial FE (first 10): {X_train.columns.tolist()[:10]}...\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "hJZLrbLidJcV",
      "metadata": {
        "id": "hJZLrbLidJcV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2: Dropping all unnecessary columns...\n",
            "Stage 2: Unnecessary columns dropped.\n",
            "X_train shape after dropping: (5348977, 28)\n",
            "X_train columns after dropping (first 10): ['amount', 'use_chip', 'merchant_state', 'zip', 'mcc', 'errors', 'client_id_card', 'card_brand', 'card_type', 'num_cards_issued']...\n"
          ]
        }
      ],
      "source": [
        "print(\"Step 2: Dropping all unnecessary columns...\")\n",
        "\n",
        "# This is the complete master list of columns to drop, consolidated from our discussions.\n",
        "final_cols_to_drop = [\n",
        "    # Identifiers and Sensitive Data\n",
        "    'id', 'client_id', 'card_id', 'merchant_id',\n",
        "    'card_number', 'cvv','mcc'\n",
        "\n",
        "    # Problematic Date Columns (and their direct derivatives if still present)\n",
        "    'acct_open_date', 'year_pin_last_changed',\n",
        "    # 'account_age_days', 'years_since_pin_change' should be gone if derived from above\n",
        "\n",
        "    # Redundant/Low-Value Categoricals/Text\n",
        "    'card_on_dark_web', # This was a single-value column, so its OHE version 'card_on_dark_web_No' would also be single-value.\n",
        "    'has_chip', # Replaced by 'has_chip_binary'\n",
        "    'address', # High cardinality text, not used for FE\n",
        "    'merchant_city', # High cardinality categorical, often redundant with zip/state\n",
        "\n",
        "    # Redundant Age Features\n",
        "    'birth_year',\n",
        "    'birth_month',\n",
        "\n",
        "    # Geospatial (dropped as I decided against complex geospatial FE)\n",
        "    'latitude',\n",
        "    'longitude',\n",
        "\n",
        "    # Original Date Columns (replaced by extracted features)\n",
        "    # These should be dropped by apply_preprocessing, but included here for robustness if they somehow remain.\n",
        "    'date', 'expires',\n",
        "    # 'hour_of_day', 'day_of_week', 'month' should also be dropped by apply_preprocessing\n",
        "]\n",
        "\n",
        "for df_set in [X_train, X_cv, X_test]:\n",
        "    # Filter list to only drop columns that actually exist in the DataFrame\n",
        "    cols_that_exist = [col for col in final_cols_to_drop if col in df_set.columns]\n",
        "    df_set.drop(columns=cols_that_exist, inplace=True, errors='ignore') # Use errors='ignore' for robustness\n",
        "\n",
        "print('Stage 2: Unnecessary columns dropped.')\n",
        "print(f\"X_train shape after dropping: {X_train.shape}\")\n",
        "print(f\"X_train columns after dropping (first 10): {X_train.columns.tolist()[:10]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ELbBxT4cdJZq",
      "metadata": {
        "id": "ELbBxT4cdJZq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3: Grouping and one-hot encoding...\n",
            "Categorical columns to encode: ['use_chip', 'merchant_state', 'errors', 'card_brand', 'card_type', 'description']\n",
            "Stage 3: Grouping and one-hot encoding complete.\n",
            "X_train shape after OHE: (5348977, 178)\n",
            "X_train columns after OHE (first 10): ['amount', 'zip', 'mcc', 'client_id_card', 'num_cards_issued', 'credit_limit', 'current_age', 'retirement_age', 'gender', 'per_capita_income']...\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "print(\"Step 3: Grouping and one-hot encoding...\")\n",
        "\n",
        "# --- Handle merchant_state: Grouping ---\n",
        "# Group merchant_state based on training set fraud counts (CRUCIAL for no data leakage)\n",
        "# This part needs to be outside the loop and use y_train.\n",
        "# Ensure 'merchant_state' is clean (NaNs filled) before this step.\n",
        "temp_train_df = pd.DataFrame({'merchant_state': X_train['merchant_state'], 'is_fraud': y_train})\n",
        "fraud_counts = temp_train_df[temp_train_df['is_fraud'] == 1]['merchant_state'].value_counts()\n",
        "top_15_fraud_states = fraud_counts.nlargest(15).index.tolist()\n",
        "del temp_train_df, fraud_counts # Clean up temporary data\n",
        "\n",
        "# Apply grouping to all splits using the list derived from X_train\n",
        "for df_set in [X_train, X_cv, X_test]:\n",
        "    if 'merchant_state' in df_set.columns:\n",
        "        # Use .loc for safe assignment\n",
        "        df_set.loc[:, 'merchant_state'] = df_set['merchant_state'].apply(lambda x: x if x in top_15_fraud_states else 'OTHER_STATE')\n",
        "\n",
        "\n",
        "# --- One-Hot Encode all remaining 'object' type columns ---\n",
        "# This includes 'merchant_state', 'errors', 'card_brand', 'card_type', 'description', 'use_chip'\n",
        "# and 'gender' if they are still object type.\n",
        "categorical_cols_to_encode = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f'Categorical columns to encode: {categorical_cols_to_encode}')\n",
        "\n",
        "# Initialize OneHotEncoder (fit only on X_train to prevent data leakage)\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=np.int8)\n",
        "encoder.fit(X_train[categorical_cols_to_encode])\n",
        "\n",
        "# Transform and concatenate for all splits\n",
        "encoded_cols_train = pd.DataFrame(encoder.transform(X_train[categorical_cols_to_encode]), index=X_train.index, columns=encoder.get_feature_names_out(categorical_cols_to_encode))\n",
        "encoded_cols_cv = pd.DataFrame(encoder.transform(X_cv[categorical_cols_to_encode]), index=X_cv.index, columns=encoder.get_feature_names_out(categorical_cols_to_encode))\n",
        "encoded_cols_test = pd.DataFrame(encoder.transform(X_test[categorical_cols_to_encode]), index=X_test.index, columns=encoder.get_feature_names_out(categorical_cols_to_encode))\n",
        "\n",
        "X_train = pd.concat([X_train.drop(columns=categorical_cols_to_encode), encoded_cols_train], axis=1)\n",
        "X_cv = pd.concat([X_cv.drop(columns=categorical_cols_to_encode), encoded_cols_cv], axis=1)\n",
        "X_test = pd.concat([X_test.drop(columns=categorical_cols_to_encode), encoded_cols_test], axis=1)\n",
        "\n",
        "gc.collect() # Clean up memory\n",
        "\n",
        "print('Stage 3: Grouping and one-hot encoding complete.')\n",
        "print(f\"X_train shape after OHE: {X_train.shape}\")\n",
        "print(f\"X_train columns after OHE (first 10): {X_train.columns.tolist()[:10]}...\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "PRCr8qezdJXY",
      "metadata": {
        "id": "PRCr8qezdJXY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4: Downcasting data types for memory efficiency...\n",
            "\n",
            "--- Preprocessing Fully Complete ---\n",
            "X_train final shape: (5348977, 178)\n",
            "X_cv final shape: (1782993, 178)\n",
            "X_test final shape: (1782993, 178)\n"
          ]
        }
      ],
      "source": [
        "print(\"Step 4: Downcasting data types for memory efficiency...\")\n",
        "for df_set in [X_train, X_cv, X_test]:\n",
        "    for col in df_set.select_dtypes(include=['float64', 'int64']).columns:\n",
        "        if 'float' in str(df_set[col].dtype):\n",
        "            df_set.loc[:, col] = df_set[col].astype('float32')\n",
        "        else:\n",
        "            df_set.loc[:, col] = pd.to_numeric(df_set[col], downcast='integer')\n",
        "gc.collect() # Clean up memory\n",
        "\n",
        "print(\"\\n--- Preprocessing Fully Complete ---\")\n",
        "print(f\"X_train final shape: {X_train.shape}\")\n",
        "print(f\"X_cv final shape: {X_cv.shape}\")\n",
        "print(f\"X_test final shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "VYMkfINddJU8",
      "metadata": {
        "id": "VYMkfINddJU8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Final NaN Check Across ALL Remaining Columns ---\n",
            "\n",
            "X_train NaNs:\n",
            "Series([], dtype: int64)\n",
            "\n",
            "X_cv NaNs:\n",
            "Series([], dtype: int64)\n",
            "\n",
            "X_test NaNs:\n",
            "Series([], dtype: int64)\n",
            "\n",
            "All DataFrames are clean (no NaNs found). Ready for modeling!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Final NaN Check Across ALL Remaining Columns ---\")\n",
        "nan_counts_xtrain = X_train.isna().sum()\n",
        "print(f\"\\nX_train NaNs:\")\n",
        "print(nan_counts_xtrain[nan_counts_xtrain > 0]) # Should be empty!\n",
        "\n",
        "nan_counts_xcv = X_cv.isna().sum()\n",
        "print(f\"\\nX_cv NaNs:\")\n",
        "print(nan_counts_xcv[nan_counts_xcv > 0])\n",
        "\n",
        "nan_counts_test = X_test.isna().sum()\n",
        "print(f\"\\nX_test NaNs:\")\n",
        "print(nan_counts_test[nan_counts_test > 0])\n",
        "\n",
        "if nan_counts_xtrain[nan_counts_xtrain > 0].empty and \\\n",
        "   nan_counts_xcv[nan_counts_xcv > 0].empty and \\\n",
        "   nan_counts_test[nan_counts_test > 0].empty:\n",
        "    print(\"\\nAll DataFrames are clean (no NaNs found). Ready for modeling!\")\n",
        "else:\n",
        "    print(\"\\nWARNING: NaNs still present in DataFrames. Please review preprocessing steps.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ljXcJ6HAdJSW",
      "metadata": {
        "id": "ljXcJ6HAdJSW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count of Legitimate Transactions (0) in y_train: 5340978\n",
            "Count of Fraudulent Transactions (1) in y_train: 7999\n",
            "Calculated scale_pos_weight: 667.71\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Model training using XGBoost Classifier\n",
        "\n",
        "# Calculate scale_pos_weight from the training target variable\n",
        "neg_count = y_train.value_counts()[0]\n",
        "pos_count = y_train.value_counts()[1]\n",
        "scale_pos_weight_value = neg_count / pos_count\n",
        "print(f\"Count of Legitimate Transactions (0) in y_train: {neg_count}\")\n",
        "print(f\"Count of Fraudulent Transactions (1) in y_train: {pos_count}\")\n",
        "print(f\"Calculated scale_pos_weight: {scale_pos_weight_value:.2f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "IeSU5_RidJPj",
      "metadata": {
        "id": "IeSU5_RidJPj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training XGBoost model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "g:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [00:25:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model training complete.\n",
            "\n",
            "--- Evaluation on Validation Set (X_cv) ---\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1744201   36125]\n",
            " [    178    2489]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99   1780326\n",
            "           1       0.06      0.93      0.12      2667\n",
            "\n",
            "    accuracy                           0.98   1782993\n",
            "   macro avg       0.53      0.96      0.56   1782993\n",
            "weighted avg       1.00      0.98      0.99   1782993\n",
            "\n",
            "\n",
            "ROC AUC Score: 0.9928\n",
            "\n",
            "Feature Importances (Top 15):\n",
            "merchant_state_Italy                                               0.123290\n",
            "description_Tolls and Bridge Fees                                  0.108466\n",
            "use_chip_Online Transaction                                        0.104134\n",
            "description_Taxicabs and Limousines                                0.031447\n",
            "zip                                                                0.030257\n",
            "merchant_state_Haiti                                               0.030215\n",
            "merchant_state_OTHER_STATE                                         0.024125\n",
            "description_Utilities - Electric, Gas, Water, Sanitary             0.024025\n",
            "description_Cable, Satellite, and Other Pay Television Services    0.018159\n",
            "mcc                                                                0.016484\n",
            "description_Service Stations                                       0.014910\n",
            "description_Miscellaneous Food Stores                              0.014325\n",
            "description_Telecommunication Services                             0.013396\n",
            "description_Family Clothing Stores                                 0.011283\n",
            "use_chip_Chip Transaction                                          0.011024\n",
            "dtype: float32\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Initialize and Train XGBoost Classifier\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',  # For binary classification\n",
        "    eval_metric='logloss',        # Metric for evaluation during training\n",
        "    use_label_encoder=False,      # Suppress a future deprecation warning\n",
        "    scale_pos_weight=scale_pos_weight_value, # Crucial for imbalance\n",
        "    random_state=42,              # For reproducibility\n",
        "    n_estimators=500,             # Number of boosting rounds (trees)\n",
        "    learning_rate=0.05,           # Step size shrinkage to prevent overfitting\n",
        "    max_depth=5,                  # Maximum depth of a tree\n",
        "    subsample=0.7,                # Subsample ratio of the training instance\n",
        "    colsample_bytree=0.7,         # Subsample ratio of columns when constructing each tree\n",
        "    gamma=0.1                     # Minimum loss reduction required to make a further partition\n",
        ")\n",
        "\n",
        "print(\"Training XGBoost model...\")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "print(\"Model training complete.\\n\")\n",
        "\n",
        "# Evaluate on Validation Set\n",
        "y_pred_cv = xgb_model.predict(X_cv)\n",
        "y_proba_cv = xgb_model.predict_proba(X_cv)[:, 1] # Probabilities for the positive class\n",
        "\n",
        "print(\"--- Evaluation on Validation Set (X_cv) ---\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_cv, y_pred_cv))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_cv, y_pred_cv))\n",
        "roc_auc = roc_auc_score(y_cv, y_proba_cv)\n",
        "print(f\"\\nROC AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Display Feature Importances\n",
        "print(\"\\nFeature Importances (Top 15):\")\n",
        "feature_importances = pd.Series(xgb_model.feature_importances_, index=X_train.columns)\n",
        "print(feature_importances.nlargest(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3kfa3oOrjnKd",
      "metadata": {
        "id": "3kfa3oOrjnKd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Identified 10 features with the lowest importance to drop for tuning.\n",
            "New shape of X_train for tuning: (5348977, 168)\n",
            "New shape of X_cv for tuning: (1782993, 168)\n",
            "New shape of X_test for tuning: (1782993, 168)\n"
          ]
        }
      ],
      "source": [
        "# Get feature importances from the model I just trained\n",
        "feature_importances = pd.Series(xgb_model.feature_importances_, index=X_train.columns)\n",
        "\n",
        "# Identify the least important features to drop for tuning.\n",
        "# I'll drop the bottom 10 features for this example. This number can be adjusted.\n",
        "low_importance_features = feature_importances.nsmallest(10).index.tolist()\n",
        "\n",
        "print(f\"Identified {len(low_importance_features)} features with the lowest importance to drop for tuning.\")\n",
        "\n",
        "# Create reduced DataFrames for the search.\n",
        "# RandomizedSearchCV will run on these smaller datasets to save time and memory.\n",
        "X_train_model2 = X_train.drop(columns=low_importance_features, errors='ignore')\n",
        "X_cv_model2 = X_cv.drop(columns=low_importance_features, errors='ignore')\n",
        "X_test_model2=X_test.drop(columns=low_importance_features, errors='ignore')\n",
        "\n",
        "del X_train,X_cv,X_test\n",
        "gc.collect()\n",
        "\n",
        "print(f\"New shape of X_train for tuning: {X_train_model2.shape}\")\n",
        "print(f\"New shape of X_cv for tuning: {X_cv_model2.shape}\")\n",
        "print(f\"New shape of X_test for tuning: {X_test_model2.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "PWyKKqasjnIX",
      "metadata": {
        "id": "PWyKKqasjnIX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Creating Interaction Features for Model 2's Reduced Datasets ---\n",
            "Interaction features created for Model 2 datasets.\n",
            "X_train_model2 shape is now: (5348977, 178)\n",
            "X_cv_model2 shape is now: (1782993, 178)\n",
            "X_test_model2 shape is now: (1782993, 178)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Creating Interaction Features for Model 2's Reduced Datasets ---\")\n",
        "\n",
        "# List of all dataframes to apply interactions to (these are the already reduced ones)\n",
        "dfs_for_interactions_model2 = [X_train_model2, X_cv_model2, X_test_model2]\n",
        "\n",
        "for df_set in dfs_for_interactions_model2:\n",
        "    # --- Amount x Top Categorical/Binary Features ---\n",
        "    # Check if base columns exist and are numeric before creating interactions.\n",
        "\n",
        "    if all(col in df_set.columns for col in ['amount', 'merchant_state_Italy']):\n",
        "        df_set['amount_x_state_italy'] = df_set['amount'] * df_set['merchant_state_Italy']\n",
        "    if all(col in df_set.columns for col in ['amount', 'description_Tolls and Bridge Fees']):\n",
        "        df_set['amount_x_tolls'] = df_set['amount'] * df_set['description_Tolls and Bridge Fees']\n",
        "    if all(col in df_set.columns for col in ['amount', 'use_chip_Online Transaction']):\n",
        "        df_set['amount_x_online_trans'] = df_set['amount'] * df_set['use_chip_Online Transaction']\n",
        "    if all(col in df_set.columns for col in ['amount', 'use_chip_Swipe Transaction']):\n",
        "        df_set['amount_x_swipe_trans'] = df_set['amount'] * df_set['use_chip_Swipe Transaction']\n",
        "    if all(col in df_set.columns for col in ['amount', 'merchant_state_Haiti']):\n",
        "        df_set['amount_x_state_haiti'] = df_set['amount'] * df_set['merchant_state_Haiti']\n",
        "    if all(col in df_set.columns for col in ['amount', 'description_Taxicabs and Limousines']):\n",
        "        df_set['amount_x_taxis_limos'] = df_set['amount'] * df_set['description_Taxicabs and Limousines']\n",
        "    if all(col in df_set.columns for col in ['amount', 'use_chip_Chip Transaction']):\n",
        "        df_set['amount_x_chip_trans'] = df_set['amount'] * df_set['use_chip_Chip Transaction']\n",
        "    if all(col in df_set.columns for col in ['amount', 'merchant_state_OTHER_STATE']):\n",
        "        df_set['amount_x_state_other'] = df_set['amount'] * df_set['merchant_state_OTHER_STATE']\n",
        "\n",
        "    # --- Other Interaction Ideas (Non-Amount Based) ---\n",
        "    if all(col in df_set.columns for col in ['credit_score', 'use_chip_Online Transaction']):\n",
        "        df_set['credit_score_x_online_trans'] = df_set['credit_score'] * df_set['use_chip_Online Transaction']\n",
        "\n",
        "    # Debt-to-Income Ratio (handle division by zero if yearly_income can be 0)\n",
        "    if all(col in df_set.columns for col in ['total_debt', 'yearly_income']):\n",
        "        # Add a small epsilon to yearly_income to prevent division by zero\n",
        "        df_set['debt_to_income_ratio'] = df_set['total_debt'] / (df_set['yearly_income'] + 1e-6)\n",
        "\n",
        "print(\"Interaction features created for Model 2 datasets.\")\n",
        "print(f\"X_train_model2 shape is now: {X_train_model2.shape}\")\n",
        "print(f\"X_cv_model2 shape is now: {X_cv_model2.shape}\")\n",
        "print(f\"X_test_model2 shape is now: {X_test_model2.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "tR5WRpBtjnFy",
      "metadata": {
        "id": "tR5WRpBtjnFy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training XGBoost model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "g:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [00:44:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model training complete.\n",
            "\n",
            "--- Evaluation on Validation Set (X_cv_model2) ---\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1744799   35527]\n",
            " [    178    2489]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99   1780326\n",
            "           1       0.07      0.93      0.12      2667\n",
            "\n",
            "    accuracy                           0.98   1782993\n",
            "   macro avg       0.53      0.96      0.56   1782993\n",
            "weighted avg       1.00      0.98      0.99   1782993\n",
            "\n",
            "\n",
            "ROC AUC Score: 0.9927\n",
            "\n",
            "Feature Importances (Top 15):\n",
            "use_chip_Online Transaction                                        0.110121\n",
            "merchant_state_Italy                                               0.106560\n",
            "description_Tolls and Bridge Fees                                  0.096222\n",
            "amount_x_state_italy                                               0.060888\n",
            "description_Taxicabs and Limousines                                0.032930\n",
            "merchant_state_Haiti                                               0.030947\n",
            "amount_x_tolls                                                     0.028758\n",
            "amount_x_state_haiti                                               0.022861\n",
            "credit_score_x_online_trans                                        0.021223\n",
            "description_Utilities - Electric, Gas, Water, Sanitary             0.018617\n",
            "description_Cable, Satellite, and Other Pay Television Services    0.015222\n",
            "description_Telecommunication Services                             0.014701\n",
            "amount_x_taxis_limos                                               0.013899\n",
            "description_Miscellaneous Food Stores                              0.013212\n",
            "description_Service Stations                                       0.012609\n",
            "dtype: float32\n"
          ]
        }
      ],
      "source": [
        "# model 2training again with xgboost classifier\n",
        "\n",
        "# Initialize and Train XGBoost Classifier\n",
        "xgb_model_2 = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',  # For binary classification\n",
        "    eval_metric='logloss',        # Metric for evaluation during training\n",
        "    use_label_encoder=False,      # Suppress a future deprecation warning\n",
        "    scale_pos_weight=scale_pos_weight_value, # Crucial for imbalance\n",
        "    random_state=42,              # For reproducibility\n",
        "    n_estimators=500,             # Number of boosting rounds (trees)\n",
        "    learning_rate=0.05,           # Step size shrinkage to prevent overfitting\n",
        "    max_depth=5,                  # Maximum depth of a tree\n",
        "    subsample=0.7,                # Subsample ratio of the training instance\n",
        "    colsample_bytree=0.7,         # Subsample ratio of columns when constructing each tree\n",
        "    gamma=0.1                     # Minimum loss reduction required to make a further partition\n",
        ")\n",
        "\n",
        "print(\"Training XGBoost model...\")\n",
        "xgb_model_2.fit(X_train_model2, y_train)\n",
        "print(\"Model training complete.\\n\")\n",
        "\n",
        "# Evaluate on Validation Set\n",
        "y_pred_cv = xgb_model_2.predict(X_cv_model2)\n",
        "y_proba_cv = xgb_model_2.predict_proba(X_cv_model2)[:, 1] # Probabilities for the positive class\n",
        "\n",
        "print(\"--- Evaluation on Validation Set (X_cv_model2) ---\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_cv, y_pred_cv))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_cv, y_pred_cv))\n",
        "roc_auc = roc_auc_score(y_cv, y_proba_cv)\n",
        "print(f\"\\nROC AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Display Feature Importances\n",
        "print(\"\\nFeature Importances (Top 15):\")\n",
        "feature_importances = pd.Series(xgb_model_2.feature_importances_, index=X_train_model2.columns)\n",
        "print(feature_importances.nlargest(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98058f26",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['xgb_model.pkl']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "\n",
        "joblib.dump(xgb_model, 'xgb_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "41903052",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['xgb_model_2.pkl']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "joblib.dump(xgb_model_2, 'xgb_model_2.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "NSxotgmTjnCY",
      "metadata": {
        "id": "NSxotgmTjnCY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Hyperparameter Tuning with Randomized Search (Memory-Optimized) ---\n",
            "Created a tuning sample of size: (2500000, 178)\n",
            "\n",
            "Running RandomizedSearchCV on SAMPLE training data...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.1, max_depth=6, n_estimators=500, subsample=0.8; total time= 4.9min\n",
            "[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.1, max_depth=6, n_estimators=500, subsample=0.8; total time= 6.2min\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRunning RandomizedSearchCV on SAMPLE training data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# FIT ON THE SAMPLE DATASET HERE:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mrandom_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_sample_for_tuning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_sample_for_tuning\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Search Complete ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# --- 4. Analyze the Results ---\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mg:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mg:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mg:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1992\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1990\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1991\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1992\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1993\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1994\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1995\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1996\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mg:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mg:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mg:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mg:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mg:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mg:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:859\u001b[39m, in \u001b[36m_fit_and_score\u001b[39m\u001b[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[39m\n\u001b[32m    857\u001b[39m         estimator.fit(X_train, **fit_params)\n\u001b[32m    858\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m         \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    862\u001b[39m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[32m    863\u001b[39m     fit_time = time.time() - start_time\n",
            "\u001b[36mFile \u001b[39m\u001b[32mg:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mg:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:1683\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1661\u001b[39m model, metric, params, feature_weights = \u001b[38;5;28mself\u001b[39m._configure_fit(\n\u001b[32m   1662\u001b[39m     xgb_model, params, feature_weights\n\u001b[32m   1663\u001b[39m )\n\u001b[32m   1664\u001b[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[32m   1665\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1666\u001b[39m     X=X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1680\u001b[39m     feature_types=\u001b[38;5;28mself\u001b[39m.feature_types,\n\u001b[32m   1681\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1683\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n\u001b[32m   1698\u001b[39m     \u001b[38;5;28mself\u001b[39m.objective = params[\u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mg:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mg:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\xgboost\\training.py:183\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mg:\\mit-capstone-project-2025\\FinSage-Financial_Insight_and_Risk_Advisor\\.venv\\Lib\\site-packages\\xgboost\\core.py:2247\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2246\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     )\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# --- Section 8: Hyperparameter Tuning with Randomized Search (Memory-Optimized) ---\n",
        "\n",
        "print(\"\\n--- Starting Hyperparameter Tuning with Randomized Search (Memory-Optimized) ---\")\n",
        "\n",
        "# --- IMPORTANT: Memory Management for Tuning ---\n",
        "# Create a smaller sample of your training data for tuning.\n",
        "# This is the key to avoiding MemoryError during cross-validation,\n",
        "# as RandomizedSearchCV will train on this smaller subset.\n",
        "SAMPLE_SIZE_FOR_TUNING = 2500000 # Adjust this size based on your RAM. 500k-1M is often a good starting point.\n",
        "                               # This will be roughly 1/10th of your training data.\n",
        "\n",
        "# Ensure y_train is aligned with the sample of X_train_model2_base\n",
        "# Use .copy() to ensure these are independent samples.\n",
        "X_train_sample_for_tuning = X_train_model2.sample(n=SAMPLE_SIZE_FOR_TUNING, random_state=42).copy()\n",
        "y_train_sample_for_tuning = y_train.loc[X_train_sample_for_tuning.index].copy() # Ensure y is aligned\n",
        "\n",
        "\n",
        "print(f\"Created a tuning sample of size: {X_train_sample_for_tuning.shape}\")\n",
        "\n",
        "\n",
        "# --- 1. Define the Parameter Grid ---\n",
        "# These are the hyperparameters I want to tune.\n",
        "param_distributions = {\n",
        "    'n_estimators': [500, 750,1000],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [5, 6, 7],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
        "    'gamma': [0, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# --- 2. Initialize the XGBoost Classifier and RandomizedSearchCV ---\n",
        "# I use the fixed parameters like scale_pos_weight from before.\n",
        "base_xgb = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    # use_label_encoder=False, # REMOVE THIS LINE (it's obsolete and causes a UserWarning)\n",
        "    scale_pos_weight=scale_pos_weight_value, # This is the value calculated before\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Set up the search\n",
        "# n_iter=10 (reduced for faster search on sample)\n",
        "# cv=3 (keep 3-fold CV for robustness on sample)\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=base_xgb,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=10,         # Number of parameter combinations to try\n",
        "    cv=3,              # 3-fold cross-validation\n",
        "    scoring='roc_auc', # The best metric for this problem\n",
        "    verbose=2,         # This will print progress updates\n",
        "    random_state=42,\n",
        "    n_jobs=1          # Use all available CPU cores\n",
        ")\n",
        "\n",
        "# --- 3. Run the Search ---\n",
        "print(\"\\nRunning RandomizedSearchCV on SAMPLE training data...\")\n",
        "# FIT ON THE SAMPLE DATASET HERE:\n",
        "random_search.fit(X_train_sample_for_tuning, y_train_sample_for_tuning)\n",
        "print(\"--- Search Complete ---\")\n",
        "\n",
        "# --- 4. Analyze the Results ---\n",
        "print(f\"\\nBest ROC AUC score found on sample: {random_search.best_score_:.4f}\")\n",
        "print(\"Best parameters found on sample:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "# Get the best model from the search (this model is trained on the sample)\n",
        "best_xgb_model = random_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ce69d7",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hFVvfd9Ij5CF",
      "metadata": {
        "id": "hFVvfd9Ij5CF"
      },
      "outputs": [],
      "source": [
        "# Evaluate the best model on the validation set (X_cv)\n",
        "print(\"\\n--- Evaluation of Best Tuned Model on Validation Set (X_cv) ---\")\n",
        "y_pred_cv_tuned = best_xgb_model.predict(X_cv_model2) # Use X_cv_reduced for evaluation\n",
        "y_proba_cv_tuned = best_xgb_model.predict_proba(X_cv_model2)[:, 1]\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_cv, y_pred_cv_tuned))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_cv, y_pred_cv_tuned))\n",
        "roc_auc_tuned = roc_auc_score(y_cv, y_proba_cv_tuned)\n",
        "print(f\"\\nROC AUC Score: {roc_auc_tuned:.4f}\")\n",
        "\n",
        "# Display Feature Importances for the best model\n",
        "print(\"\\nFeature Importances (Top 20 from Tuned Model):\")\n",
        "feature_importances_tuned = pd.Series(best_xgb_model.feature_importances_, index=X_train_model2.columns) # Use X_train_model2 columns\n",
        "print(feature_importances_tuned.nlargest(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m_puIbQ-j4-_",
      "metadata": {
        "id": "m_puIbQ-j4-_"
      },
      "outputs": [],
      "source": [
        "# --- AFTER the evaluation of best_xgb_model on X_cv_model2 ---\n",
        "\n",
        "print(\"\\n--- Evaluation of Best Tuned Model with Custom Threshold (0.7) ---\")\n",
        "\n",
        "# Define the new custom threshold\n",
        "custom_threshold = 0.7\n",
        "\n",
        "# Apply the custom threshold to the probabilities from the best tuned model\n",
        "# If the probability of fraud (y_proba_cv_tuned) is greater than the custom_threshold,\n",
        "# classify as 1 (fraud), otherwise classify as 0 (not fraud).\n",
        "y_pred_cv_thresholded = (y_proba_cv_tuned > custom_threshold).astype(int)\n",
        "\n",
        "print(f\"\\nConfusion Matrix (Threshold = {custom_threshold}):\")\n",
        "print(confusion_matrix(y_cv, y_pred_cv_thresholded))\n",
        "\n",
        "print(f\"\\nClassification Report (Threshold = {custom_threshold}):\")\n",
        "print(classification_report(y_cv, y_pred_cv_thresholded))\n",
        "\n",
        "# ROC AUC is threshold-independent, so it won't change by adjusting the prediction threshold.\n",
        "# It's still useful to print it to confirm the model's underlying discriminative power.\n",
        "roc_auc_thresholded = roc_auc_score(y_cv, y_proba_cv_tuned) # Note: Still uses probabilities, not thresholded predictions\n",
        "print(f\"\\nROC AUC Score (Threshold = {custom_threshold}): {roc_auc_thresholded:.4f}\")\n",
        "\n",
        "print(\"\\nAnalysis of Custom Threshold:\")\n",
        "print(f\"By increasing the classification threshold to {custom_threshold}, I expect to see a significant decrease in False Positives (leading to higher Precision) and potentially a decrease in True Positives (leading to lower Recall). This trade-off is crucial for aligning the model's output with specific business requirements, such as reducing the number of false alarms for fraud investigation teams.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pAfTZ69Hj47W",
      "metadata": {
        "id": "pAfTZ69Hj47W"
      },
      "outputs": [],
      "source": [
        "# --- AFTER the evaluation of best_xgb_model on X_cv_model2 ---\n",
        "\n",
        "print(\"\\n--- Evaluation of Best Tuned Model with Custom Threshold (0.15) ---\")\n",
        "\n",
        "# Define the new custom threshold\n",
        "custom_threshold = 0.15\n",
        "\n",
        "# Apply the custom threshold to the probabilities from the best tuned model\n",
        "# If the probability of fraud (y_proba_cv_tuned) is greater than the custom_threshold,\n",
        "# classify as 1 (fraud), otherwise classify as 0 (not fraud).\n",
        "y_pred_cv_thresholded = (y_proba_cv_tuned > custom_threshold).astype(int)\n",
        "\n",
        "print(f\"\\nConfusion Matrix (Threshold = {custom_threshold}):\")\n",
        "print(confusion_matrix(y_cv, y_pred_cv_thresholded))\n",
        "\n",
        "print(f\"\\nClassification Report (Threshold = {custom_threshold}):\")\n",
        "print(classification_report(y_cv, y_pred_cv_thresholded))\n",
        "\n",
        "# ROC AUC is threshold-independent, so it won't change by adjusting the prediction threshold.\n",
        "# It's still useful to print it to confirm the model's underlying discriminative power.\n",
        "roc_auc_thresholded = roc_auc_score(y_cv, y_proba_cv_tuned) # Note: Still uses probabilities, not thresholded predictions\n",
        "print(f\"\\nROC AUC Score (Threshold = {custom_threshold}): {roc_auc_thresholded:.4f}\")\n",
        "\n",
        "print(\"\\nAnalysis of Custom Threshold:\")\n",
        "print(f\"By decreasing the classification threshold to {custom_threshold}, I expect to see a significant decrease in False Negatives (leading to higher Recall) and potentially a increase in True Positives (leading to lower Precidsion). This trade-off is crucial for aligning the model's output with specific business requirements, such as reducing the number of false alarms for fraud investigation teams.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vXkCUft9j44W",
      "metadata": {
        "id": "vXkCUft9j44W"
      },
      "outputs": [],
      "source": [
        "# Get the results from RandomizedSearchCV\n",
        "results = random_search.cv_results_\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Plotting function to visualize tuning results\n",
        "def plot_tuning_results(results_df, param_name, title):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Get the mean test score for each combination\n",
        "    scores = results_df['mean_test_score']\n",
        "\n",
        "    # Get the parameter values\n",
        "    param_values = results_df[f'param_{param_name}']\n",
        "\n",
        "    # Check if the parameter is a numerical type before plotting\n",
        "    if pd.api.types.is_numeric_dtype(param_values):\n",
        "        # Sort values for a cleaner plot\n",
        "        sorted_results = results_df.sort_values(by=f'param_{param_name}')\n",
        "\n",
        "        plt.plot(sorted_results[f'param_{param_name}'], sorted_results['mean_test_score'], marker='o')\n",
        "\n",
        "    else:\n",
        "        # If not numeric (e.g., a string), plot a swarmplot or similar\n",
        "        sns.swarmplot(x=f'param_{param_name}', y='mean_test_score', data=results_df)\n",
        "\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(param_name, fontsize=12)\n",
        "    plt.ylabel('Mean ROC AUC Score', fontsize=12)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Let's visualize the performance against one or two key parameters\n",
        "# Choose from ['n_estimators', 'learning_rate', 'max_depth', 'subsample', 'colsample_bytree', 'gamma']\n",
        "\n",
        "# Plot ROC AUC vs. learning_rate\n",
        "plot_tuning_results(results_df, 'learning_rate', 'Tuning Results: ROC AUC vs. Learning Rate')\n",
        "\n",
        "# Plot ROC AUC vs. max_depth\n",
        "plot_tuning_results(results_df, 'max_depth', 'Tuning Results: ROC AUC vs. Max Depth')\n",
        "\n",
        "print(\"The plots above show how performance varied with different parameter settings during the search.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k5747vzxj-m1",
      "metadata": {
        "id": "k5747vzxj-m1"
      },
      "outputs": [],
      "source": [
        "# Make sure to use the best_xgb_model obtained from RandomizedSearchCV\n",
        "# And use X_test_reduced if you applied the low-importance feature dropping.\n",
        "\n",
        "print(\"\\n--- Final Evaluation on Test Set (X_test) ---\")\n",
        "\n",
        "y_pred_test = best_xgb_model.predict(X_test_model2) # Use X_test_reduced for evaluation\n",
        "y_proba_test = best_xgb_model.predict_proba(X_test_model2)[:, 1]\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_test))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "roc_auc_test = roc_auc_score(y_test, y_proba_test)\n",
        "print(f\"\\nROC AUC Score: {roc_auc_test:.4f}\")\n",
        "\n",
        "# Display Feature Importances for the best model on the test set\n",
        "print(\"\\nFeature Importances (Top 20 from Best Model):\")\n",
        "feature_importances_final = pd.Series(best_xgb_model.feature_importances_, index=X_train_model2.columns)\n",
        "print(feature_importances_final.nlargest(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w9-CN6pqj-kG",
      "metadata": {
        "id": "w9-CN6pqj-kG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "Sq9d5txHkTMr",
      "metadata": {
        "id": "Sq9d5txHkTMr"
      },
      "source": [
        "Despite the strong performance, there's always room for further exploration and improvement in a real-world fraud detection system:\n",
        "\n",
        "Threshold Optimization: Implement a detailed threshold tuning process (e.g., plotting Precision-Recall curves) to find the optimal balance between precision and recall based on specific business costs of false positives vs. false negatives. This is crucial for practical deployment.\n",
        "\n",
        "Advanced Feature Engineering:\n",
        "\n",
        "Explore more complex velocity features (e.g., number of transactions per card/client/merchant in the last hour/day/week).\n",
        "Investigate advanced geospatial features if more granular location data is available.\n",
        "\n",
        "Other Models: Experiment with other advanced ensemble models (e.g., CatBoost) or deep learning approaches for tabular data.\n",
        "\n",
        "Anomaly Detection: Integrate unsupervised anomaly detection techniques to identify novel fraud patterns that supervised models might miss.\n",
        "\n",
        "Cost-Sensitive Learning: Directly incorporate the financial costs of misclassifications into the model's loss function.\n",
        "\n",
        "Real-time Considerations: For a production system, consider aspects like model latency, data streaming, and continuous learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6Ef-xYZ8j-e-",
      "metadata": {
        "id": "6Ef-xYZ8j-e-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m7EQrs0Zj-b2",
      "metadata": {
        "id": "m7EQrs0Zj-b2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
